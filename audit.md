PROMPT 01 — Schema Validation Audit
Goal: Ensure every component’s configuration schema (Config.schema.json) conforms to the platform’s schema standards and matches AWS MCP’s component schema model. This guarantees all required fields and structure are present for consistent validation. Instructions:
Collect Schemas: Iterate through the packages/components/* directories in the Shinobi repo and locate each Config.schema.json file. Load each JSON schema.
Load Reference Schema: Access the AWS MCP project’s standard component schema (e.g. the component.schema.json definition from AWS MCP) or use AWS MCP CLI tools if available to retrieve the expected schema structure.
Validate Structure: For each component schema, compare its structure against the AWS MCP reference:
Verify $schema is declared (e.g. draft-07) and a proper title is set (e.g. “<ComponentName> Config”).
Ensure the schema type is "object" and that top-level keys like properties, required, and sub-property definitions exist.
Check that every config field has a type and (ideally) a description. All required fields in the component’s TypeScript interface should appear under required.
Identify Deviations: Note any schema that is missing fields present in the MCP reference or violates conventions (e.g. no $schema declaration, using incorrect data types, missing descriptions, etc.). Use a JSON Schema validator if possible to catch errors.
Report Findings: For each component, record whether its Config.schema.json passes validation. List any discrepancies (e.g. missing required attributes or additional properties not allowed by MCP’s schema model). Highlight components with schema misalignment or incomplete definitions.
Deliverable: reports/schema_validation_audit.md – A Markdown report listing each component and the result of schema validation (e.g. “PASS” for compliance or specific issues if “FAIL”), with details of any mismatches or omissions per schema.
PROMPT 02 — Tagging Standard Audit
Goal: Verify that all AWS resources created by platform components are tagged according to the platform tagging standards (e.g. mandatory tags like platform:service-name, platform:environment, etc.). Ensure tagging is applied uniformly via code and no resources lack the required tags, in alignment with governance policies. Instructions:
Examine Tagging Mechanism: Inspect the base component implementation (e.g. BaseComponent._applyStandardTags() in the platform core) to understand how standard tags are applied. Confirm which tag keys are mandatory (owner, service, component name/type, environment, commit hash, etc.).
Scan Component Code: For each component’s synth() method (in *.component.ts), check that after creating each AWS CDK resource (e.g. new s3.Bucket, new rds.DatabaseInstance, etc.), the code calls the inherited tagging helper (e.g. _applyStandardTags(resource, ...)). Identify any component that creates resources without invoking the standard tagging function.
Tag Propagation: Ensure that if tags are applied at the stack or construct level, all child resources inherit them. Verify no component circumvents this (e.g. creating resources in a separate stack or outside the tagged scope). If applicable, use AWS MCP or CDK tools to detect any untagged resources in synthesized templates (for a sample service manifest).
Format & Values: Confirm that tag keys use the correct format (kebab-case) and values are filled from context (e.g. service name, env name). Also verify compliance tags: if complianceFramework is FedRAMP, check that tags like compliance:framework and compliance:ssp-id would be applied (these may be set in the root stack or environment config).
Report Gaps: List any components that do not adhere to tagging standards (e.g. missing a call to _applyStandardTags on a resource or missing specific tags). Include details on which resource or component is affected. If all components properly apply tags, confirm that and note any additional improvements (e.g. ensuring data-classification tags are provided where required by policy).
Deliverable: reports/tagging_audit.md – A Markdown report summarizing the tagging compliance of each component. It should list components or resource types that are missing tags or using non-standard tag keys/values, or state that all components correctly implement the tagging standard if no issues are found.
PROMPT 03 — Logging Standard Audit
Goal: Confirm that the platform’s logging practices are implemented across all components. This includes use of structured logging (no bare console.log), appropriate log retention settings for CloudWatch logs, and integration of correlation IDs or trace information as per observability standards. Instructions:
Structured Logging Usage: Search the codebase for any usage of console.log or other unstructured logging. Instead, verify that components and services use the platform’s logging utility (e.g. a Logger class from @shinobi/core or AWS Powertools logger) to produce JSON or structured log entries. Identify any direct print/console calls that should be replaced by structured logs.
Log Retention Configuration: Inspect all places where CloudWatch Log Groups are created by the platform (e.g. Lambda functions, API Gateway, Step Functions if any). Check that a retention period is explicitly set (e.g. 14 or 30 days for prod) instead of leaving it at “Never Expire”. In the CDK constructs (e.g. Lambda), look for the logRetention property or RetentionDays settings. List any components that do not configure log retention on their logs or on AWS services that produce logs (API Gateway, etc.).
Correlation and Context: Determine if the logging includes contextual metadata (trace or request IDs). For example, check if the Logger automatically injects AWS X-Ray trace IDs or request IDs into log entries, or if code explicitly logs such fields. Search for evidence of correlation IDs (e.g. traceId, requestId) being captured or logged. If the platform standard dictates including these (as per FedRAMP observability requirements), flag any missing implementation.
Audit FedRAMP Logging Requirements: If the platform runs in a FedRAMP mode, ensure that additional controls are met (from standards): e.g. audit log format includes timestamp, user, etc., and that 90-day retention is configured for high compliance environments. Note if these are handled via config or policy (like cdk-nag rules) and whether they are satisfied in code.
Summarize Findings: Document any deviations from the logging standard: e.g. usage of unstructured logging, missing retention settings, or lack of trace correlation. Also highlight good practices observed (e.g. “All Lambda functions use structured logging with retention = 30 days”). Provide file references for any code that needs changes (if found via search).
Deliverable: reports/logging_audit.md – A Markdown report detailing the compliance of the platform with logging standards. It should list any components or modules that require logging improvements (with specifics, like “XYZ component uses console.log” or “Log group retention not set for ABC resource”), as well as confirming areas that meet standards.
PROMPT 04 — Observability Standard Audit
Goal: Ensure that observability features (distributed tracing, metrics, and telemetry) are built into the platform components. Verify that AWS X-Ray tracing is enabled where applicable, AWS Distro for OpenTelemetry (ADOT) is integrated for metrics/traces, and that the platform is prepared to emit useful metrics for monitoring. Instructions:
X-Ray Tracing Enablement: Check all AWS Lambda-based components (e.g. lambda-api, lambda-worker) for X-Ray tracing configuration. In their CDK construct instantiation, confirm that tracing: lambda.Tracing.ACTIVE (or equivalent) is set so that invocations are traced. If any Lambda construct is missing this property, flag it. Similarly, if any API Gateway or Step Function exists, ensure X-Ray is enabled.
OpenTelemetry Integration: Verify that components include instrumentation for telemetry:
For Lambda components, see if the ADOT Lambda Layer is added (e.g. searching for a layer ARN or module like “AWSObservability” or configuration in the Lambda environment variables to enable tracing).
For container/ECS components (if any exist or when planned), check design docs or code for injecting an OTel sidecar or agent.
For EC2-based components (if present or future), confirm there’s a plan to install the CloudWatch/ADOT agent in User Data.
Custom Metrics and Alarms: Determine if components publish any custom CloudWatch metrics (e.g. throughput, latency) or if the platform has out-of-the-box alarms/dashboards. Check for usage of CDK constructs like Metric or Alarm in core services. If not implemented, ensure that the observability plan (maybe in docs or backlog) is noted.
Log and Trace Correlation: Confirm that there is a strategy to correlate logs and traces (e.g. the logger adds the current X-Ray Trace ID to each log entry). If the codebase uses AWS Lambda Powertools or a custom logger, check its implementation for injecting correlation IDs. If this is missing, mark it as an enhancement needed for full observability compliance.
Document Compliance: For each observability aspect (tracing, metrics, instrumentation), note whether it’s Implemented, Partially Implemented, or Not Implemented. Use AWS’s observability best practices as a reference (e.g. every request should be traceable and monitored). Highlight any gaps, such as “ECS component not yet implemented with sidecar (planned)” or “Lambda tracing enabled = YES”.
Deliverable: reports/observability_audit.md – A Markdown report describing how well the platform meets observability standards. It should be organized by feature (tracing, telemetry, metrics) with findings under each (for example: Tracing: “All Lambdas have X-Ray active” or “Missing X-Ray on X component”; Metrics: “No custom metrics emitted yet”; Telemetry: “ADOT Layer added to Lambdas by default”). Include suggestions for any uncovered gaps.
PROMPT 05 — CDK Best Practices Audit
Goal: Assess the codebase against AWS CDK best practices. This includes proper use of high-level constructs (avoiding low-level Cfn calls where not needed), consistent CDK version usage, presence of automated checks like cdk-nag, and avoidance of anti-patterns. The aim is to ensure the infrastructure code is idiomatic, maintainable, and secure by default. Instructions:
Construct Usage: Scan the repository for direct use of low-level Cfn* constructs or raw CloudFormation in components. Ideally, components should use L2/L3 constructs (from aws-cdk-lib) or AWS Solutions Constructs. If any CfnXXX classes are used, verify they are necessary (e.g. no higher-level construct exists) and properly configured. Flag any usage that could be replaced with safer abstractions.
CDK Version Consistency: Open the root package.json or individual package.json files to confirm all modules rely on AWS CDK v2 (no mix of v1). Check that all @aws-cdk libraries have the same version. This prevents version mismatches. Note any outdated or mismatched versions of CDK or other dependencies (you can run npm ls or pnpm list if available, or rely on package.json data).
Nag and Lint: Verify that the project integrates cdk-nag or equivalent static analysis for AWS best practices. Check if NagSuppressions are used in code – if so, each should include a justification and reference (as required by internal standards). If possible, run cdk-nag (or svc plan) on a sample service to see if any high-severity findings are present. The audit should confirm that critical issues (unencrypted resources, public access) are either not present or are caught by nag rules with proper handling.
Resource Policies and Defaults: Ensure the code sets secure defaults provided by CDK when available. For example, check that S3 buckets use blockPublicAccess and have proper removal policies, or that Secrets are secret-managed (no plaintext secrets in code). Ensure no hardcoded ARNs or environment-specific values in constructs (all should come from config). This overlaps with compliance but is a CDK usage aspect (like using RemovalPolicy.RETAIN for persistent data by default, etc.). Note any areas where CDK best practices (like explicit RemovalPolicy or enabling encryption on constructs) are not followed.
Error Handling & Warnings: Look at build/test outputs if available (perhaps via CI config or pnpm run lint). Ensure there are no ignored TypeScript compiler errors or linter warnings that could hide issues. Also verify that all synthesizable components are added to the app (no “dead code” components not registered).
Summarize Compliance: Provide an overview of adherence to CDK best practices: e.g. “CDK v2 is used consistently – OK”, “No unexpected Cfn calls – OK” or list specific refactoring suggestions (“Use higher-level construct X instead of CfnY in component Z”). Include any cdk-nag findings that should be addressed or already are addressed with justification.
Deliverable: reports/cdk_best_practices_audit.md – A Markdown report covering the above aspects. Structure it by sub-topic (Construct usage, Dependency versions, cdk-nag compliance, etc.) and list observations. For any best-practice violations, provide the file reference and recommended fix (for instance, “Use s3.Bucket with blockPublicAccess instead of CfnBucket in packages/components/abc”). If everything aligns well with CDK best practices, explicitly state that for each category.
PROMPT 06 — Component Versioning & Metadata Audit
Goal: Verify that each platform component is correctly versioned and that metadata (like version numbers, descriptions, and lifecycle status) are consistent across code and documentation. Ensure the component registry aligns with AWS MCP’s expectations by providing unique semantic versions for each component type and that changes are tracked via semantic versioning. Instructions:
Gather Component Versions: Traverse the packages/components directory and open each component’s package.json. Record the name (component type) and version field. All components should follow Semantic Versioning (X.Y.Z). Confirm no version is missing or obviously incorrect (e.g. “0.0.0” placeholder).
Consistency Check: Ensure that these version numbers match what the platform would advertise. For example, if the MCP server’s component catalog (when implemented) should list {type, version} for each component, verify that the source of truth is the package versions. There should be no conflicting versions (two different components accidentally sharing a version number is fine, but a single component should have one canonical version).
Changelog & Changesets: If the repository uses Changesets for version bumping, check that recent changes to components come with version increments according to SemVer (MAJOR for breaking, MINOR for new features, PATCH for fixes). You can inspect the CHANGELOG.md or changeset files for a couple of components to see if version updates align with changes. Note any component that was modified without a version bump (which might indicate a versioning lapse).
Metadata Fields: Beyond version, confirm other metadata: each component’s package.json should have a description. Also, in documentation (each component’s README), ensure the description and usage example are present and up-to-date. Optionally, confirm that the README lists the component’s capabilities and outputs (per contribution guidelines). This audit step ensures documentation matches the component’s current state and version (e.g. no outdated info after updates).
AWS MCP Alignment: AWS’s MCP interface expects the platform to provide component type and version in queries (e.g. GET /platform/components). Using AWS MCP as a reference for contract, verify that the platform is prepared to present these correctly. If possible, simulate or inspect the MCP server output for components (it might be stubbed now). The audit should ensure that whenever the MCP server is fully implemented, it will pull the correct version metadata (e.g. via a registry or by reading package versions). If the MCP spec requires additional metadata (like component release notes or lifecycle stage), note if Shinobi provides that (perhaps via tags in package.json like alpha/beta if any).
Report Discrepancies: List any component where the versioning seems off (e.g. not bumped when it should have, or still 0.x if it’s supposed to be stable). Also note if any component’s documentation is missing or if any version inconsistency exists between code and docs. If all components have proper semantic versions and metadata, state that and highlight the system in place (e.g. “All components versioned via Changesets, following SemVer – compliant”).
Deliverable: reports/component_versioning_audit.md – A Markdown report, possibly tabulating each component type and its version, or summarizing in text. It should call out if any component is not following the versioning scheme or if metadata is incomplete. Include observations about the overall version management process (e.g. “Versions are consistent and appear to be managed by automated tooling – no issues found” or “Component X was changed recently but remains at version 1.0.0, consider bumping for new features”).
PROMPT 07 — Configuration Precedence Chain Audit
Goal: Validate that the platform implements the configuration layering as per the Configuration Precedence Chain standard. This ensures that defaults and overrides are applied in the correct order (Layers 1–5) and that no environment-specific logic is hardcoded, aligning with the principle of “configuration over code.” Instructions:
Review Config Builders: Open a few representative *ConfigBuilder.ts classes in packages/components/* (for example, rds-postgres.builder.ts or similar). Identify how they implement the five layers:
Layer 1: getPlatformDefaults() (hardcoded safe defaults). Ensure no disallowed values (e.g. no hardcoded “prod” settings like open CIDRs or * in CORS). If present, flag them as violations of the no-environment-specific rule.
Layer 2: Loading of global platform config (e.g. commercial.yml, fedramp-high.yml). Confirm that the builder loads the correct file based on complianceFramework. Check that these config files exist under /config and contain expected defaults (like stricter settings for FedRAMP).
Layer 3: Service-level env overrides – these come from the service manifest’s environments: block. In code, verify that the builder merges environment-specific settings (the builder likely receives a context that includes environment overrides). No action needed if this is handled in base classes, but ensure it’s not omitted.
Layer 4: Component overrides – confirm that if a manifest specifies components[i].overrides, the builder applies these last (except policy). This should override even compliance defaults if set. Check builder logic or tests for precedence order (there might be unit tests like *builder.test.ts ensuring this).
Layer 5: Policy overrides – these are special cases for compliance exceptions. Confirm the builder or platform can handle policy.overrides (you might see it in IComponentCreator.validateSpec or somewhere). If policy overrides are supposed to be allowed, ensure they would trump others only when explicitly permitted.
Search for Hardcoded Env Logic: Do a keyword search in the code for environment names or stages (e.g. “prod”, “dev”, “stage”) to catch any hardcoded conditionals. None should exist in component code; all env differences should funnel through config layers. If any such if (env === ...) logic is found in the repo (for example in older components or hacks), highlight it as a violation.
Verify Merging Order: If possible, find or write a small test (or use an existing unit test) to simulate a component config resolution. For example, ensure that a value set in a component’s overrides in service.yml will indeed override a value from layer 2 (platform default). The presence of unit tests in builder.test.ts for different frameworks and override scenarios is a good sign – skim those tests (if present) to confirm they cover precedence (e.g. a test that component override > env default > platform default).
Compliance Segregation: Confirm that the platform segregates config by framework – e.g. when synthesizing a commercial deployment, it does not accidentally apply FedRAMP settings. This might involve ensuring that the code only loads one framework’s YAML based on context. Note how the code determines which config file to load (likely via complianceFramework field in the manifest). If multiple frameworks could be mixed erroneously, that’s a bug; if logic is clear (e.g. switch on framework), that’s correct.
Outcome: Summarize whether the configuration precedence is implemented correctly. Note any instances of mis-order (e.g. if compliance defaults might override user overrides incorrectly) or any hardcoded values that violate “no environment-specific logic in code.” Also highlight if all prohibited sensitive defaults are avoided (e.g. no wildcard CORS by default, etc., as per the standard’s Section 3.1).
Deliverable: reports/configuration_precedence_audit.md – A Markdown document describing the findings. It should state if the layering (Layer 1–5) is honored by the implementation, and list any rule violations (like environment-specific hardcoding or incorrect override behavior). Use bullet points or a short section per layer to report compliance (e.g. “Layer 1 (Platform Defaults): Safe defaults present, no env-specific values hardcoded, Compliant” or noting issues). Include references to any problematic code lines for clarity.
PROMPT 08 — Capability Binding & Binder Matrix Audit
Goal: Ensure that the platform’s Capability declarations and Binder Matrix are consistent and comprehensive. Each component should declare the capabilities it provides, and the binder strategies should cover all supported source–target combinations as per AWS MCP’s capability registry expectations. This audit checks that components and binders speak the same “language” of capabilities (e.g. db:postgres, queue:sqs) and that no valid binding scenario is missing or misconfigured. Instructions:
Inventory of Capabilities: Compile a list of all capability strings used in Shinobi. These appear in two places:
Binder Strategies: Open the binder strategy definitions or docs (e.g. docs/BINDER_STRATEGY_SYSTEM.md or the strategy classes) to see all capability names (e.g. db:postgres, cache:redis, storage:s3, etc. including any aliases).
Component Declarations: Search the component code for _registerCapability( calls. Each component that provides a bindable output should register a capability with some data (e.g. an RDS component registering db:postgres with its endpoint info). Note which capability each component provides.
Name Alignment: Verify that capability naming follows the standard category:subtype format, all lowercase, matching AWS service domains (for example, database categories start with db: for relational DBs). Cross-check against any AWS MCP guidance on capability naming (if AWS MCP doesn’t dictate, use Shinobi’s standard as source of truth). Ensure consistency: e.g. no mix of cache:redis vs cache:Redis (case should be consistent, etc.).
Component vs Capability Map: Ensure every capability in the binder matrix has a corresponding component that provides it:
For each capability listed in binder strategies, confirm at least one component type is meant to supply that capability. (e.g. db:postgres -> provided by rds-postgres component, cache:redis -> provided by elasticache-redis component, etc.)
Conversely, if a component claims a capability, ensure the binder system knows how to handle it. If you find a component registering a capability not covered in any binder strategy, that’s a gap (means nothing can bind to it). Also flag if any capability synonyms exist (like storage:s3 vs bucket:s3) to ensure the binder covers them interchangeably or the component sticks to one canonical name.
Binder Matrix Completeness: Examine the BinderRegistry or matrix of supported bindings (source -> target). Check that all logical bindings are present and marked supported:
For example, a Lambda function component should be allowed to bind to an SQS queue (source lambda, target capability queue:sqs – likely covered by QueueBinderStrategy), and to a database (db:postgres via DatabaseBinderStrategy), etc. Using AWS MCP’s notion of capabilities, ensure the platform doesn’t miss obvious links.
Identify any missing binding: e.g. if a new component type exists and should connect to another but no strategy is defined, note it. The documentation’s binder list should enumerate supported pairs; verify against implemented strategies.
Data Contract Consistency: For a couple of capabilities, compare the data that components provide vs what binders expect. For instance, the Database binder expects certain fields (host, port, secretArn, etc. as per docs). Check the RDS component’s _registerCapability("db:postgres", data) to see if it supplies all those fields in its data payload. If any required piece is missing or named differently, that could cause runtime issues. Do this for one or two key capability types (DB, cache, etc.).
Results: Summarize whether the capability naming and binding system is consistent and complete. Note any discrepancies such as: capability names not standardized, a binder for capability X missing, or a component providing Y but binder expecting a different key. Also mention if everything appears aligned (e.g. “All defined capabilities have matching binder strategies and providing components. Capability naming follows standard conventions.”).
Deliverable: reports/capability_binder_audit.md – A Markdown report detailing the audit. It may include a table mapping components to capabilities and indicating if binder support exists, or sections like “Supported Capabilities & Components” and “Binder Matrix Gaps”. Include any recommendations, e.g. “Add binder for <new capability> if a new component is planned” or “Rename capability X for consistency with AWS naming”. If everything is in order, the report should confirm that each capability is well-supported by the binder matrix and components.
PROMPT 09 — Internal Dependency Graph Audit
Goal: Analyze the internal dependency structure of the platform’s code to ensure a clean, modular architecture. The platform’s packages (core, contracts, components, etc.) should not have unintended circular dependencies, and components should remain decoupled (no component directly instantiating another inside its code). This audit checks that the layering is as expected and aligns with AWS’s guidance for modular design. Instructions:
Module Layering: Outline the intended dependency flow: for Shinobi, likely @platform/contracts (interfaces) and @platform/core can be common dependencies for component packages, but components should not depend on each other. Verify this by inspecting each component’s package.json dependencies: they should include @platform/contracts and possibly @platform/core, but generally not another component. If any component lists another component as a dependency, flag it (this could indicate undesired coupling or shared code that should be refactored to core).
Check for Cycles: Using a dependency analysis tool or manual inspection, ensure there are no circular dependency loops. For example, core depending on components would be a red flag. Check the import statements in core/contract packages to ensure they don’t import from specific components (search in packages/core for strings like “components/” to be sure). Also ensure that components only use the contracts/core APIs rather than reaching into each other’s implementation.
Cross-Component Calls: Search the component code for any instance of one component class using another component’s class or resources directly. For example, a component should not call new SomeOtherComponent(); interactions should happen via binds (which are handled by the platform resolver, not direct code calls). If you find code that creates or manipulates another component within a component (unlikely by design), highlight it as a violation of encapsulation.
Shared Utilities: Identify if there are any utility or common modules that all components use (besides core/contracts). If there are separate packages (like a @platform/common or similar), ensure they are used appropriately and do not introduce cycles. For instance, if a utility depends on components, that’s problematic. Ensure common code is placed in lower-level modules that components depend on, not vice versa.
AWS MCP Reference (Dependency Graph): The platform MCP server even exposes a dependency graph of services and components (via shinobi://dependencies). That implies internally the platform knows how components connect (through binds). Ensure that this representation is built purely from manifest binds, which keeps components independent. (No action needed if so, but mention that components rely on the Resolver/Binder to handle dependencies, which is correct).
Report Architecture Health: Summarize whether the codebase’s dependency structure is clean. e.g. “No circular dependencies detected among packages. Core and Contracts are independent base layers; all components depend only on them, which is as expected.” Or if any issues: e.g. “Found a dependency from component X to component Y – consider moving shared logic to core.” If tools were used (like madge or similar for cycle detection), you can mention results.
Deliverable: reports/dependency_graph_audit.md – A Markdown report describing the internal dependency relationships and any issues. It should call out any unexpected dependencies or confirm that the layering (contracts -> core -> components) is maintained with no cycles. This can include a brief list of packages and their deps or simply a narrative evaluation. Make clear whether the architecture adheres to the desired decoupling (which it should, if all is well).
PROMPT 10 — MCP Server API Contract Audit
Goal: Determine if the Shinobi MCP server implementation aligns with the expected Model Context Protocol (MCP) API contract. According to the platform spec (and AWS’s MCP guidelines), the MCP server should expose endpoints (or commands) like listing components, retrieving component schemas, listing bindings/capabilities, validating manifests, etc. This audit checks that those endpoints exist, are correctly implemented, and return data consistent with the spec (not placeholders), highlighting any missing pieces needed for full AWS MCP compatibility. Instructions:
Cross-Reference Spec: Use the “Platform MCP Server Specification” (docs/spec/platform-mcp-spec.md) as the baseline. Note the key endpoints it defines:
Platform-level: /platform/components, /platform/components/{type}/schema, /platform/capabilities, /platform/bindings, /platform/validate (and possibly /platform/health if any).
Service-level: /services, /services/{name}, /services/{name}/manifest, etc., though focus on platform-level for now.
Inspect Implementation: Open the apps/shinobi-mcp-server/src/shinobi-server.ts (MCP server code) and identify handlers corresponding to those endpoints:
Check if there is a handler for listing components (it might be implemented as a getComponentCatalog tool or responding to resource URI shinobi://components). See if it currently returns real data or just a stub message.
Verify there’s a way to get a specific component’s schema (likely getComponentSchema exists as a command). Determine if it reads the actual JSON schema file or is a stub.
Look for any implementation of listing capabilities or the binding matrix. The spec’s /platform/capabilities and /platform/bindings should correspond to something like a shinobi://capabilities resource or a tool command, but scanning the code, see if such exists. If not, mark those endpoints as not yet implemented.
Confirm manifest validation endpoint: The spec’s /platform/validate likely maps to a command (perhaps lint_manifest or similar). Check if CallToolRequestSchema handlers include a validate or lint_manifest tool. If yes, see if it invokes the platform’s validation logic (schema validation pipeline). If no explicit validate command, note that as a gap.
Data Format Check: For the implemented endpoints (components, services, dependencies, etc.), ensure the output structure matches the spec’s examples. E.g., /platform/components should return an array of objects with type and version. If the current implementation just returns a text blob or incomplete data, that’s an issue. From the code, determine the current output: (e.g., getComponentCatalog is currently returning a placeholder string rather than an array of components). Mark this as needing completion.
Similarly, the dependency graph (shinobi://dependencies) returns nodes and edges (we saw a sample static graph). The spec doesn’t explicitly detail that endpoint but presumably the MCP server should provide it – Shinobi does via a stub static example. Note whether this should be dynamic or is acceptable.
The /platform/bindings (binding matrix) – check if shinobi://dependencies or another resource covers it. Possibly the platform expects clients to derive binding rules via capabilities; but since spec listed /platform/bindings returning supported bindings, if it’s not implemented, mention it.
Identify Gaps: List any spec endpoints not implemented or only stubbed: e.g. “Component list and schema retrieval exist but currently return dummy data – need to implement actual registry lookup. The capabilities listing endpoint is missing entirely.” Use AWS MCP’s own project expectations (if any standard) to support why these are needed (for AI agents to fully leverage the platform).
Security & Auth (if any): The spec mentions authentication, but in code we might not see it yet. If no auth mechanism is present in the MCP server (open by default), note that as a deviation from “Secure by Design” principle (though it might be okay in dev mode for now).
Conclusion: Evaluate how ready the MCP server is for AI clients. Ideally, after this audit, we know which parts of the contract are fulfilled and which need work. Summarize this readiness.
Deliverable: reports/mcp_contract_audit.md – A Markdown report detailing the comparison of the Shinobi MCP server against the expected MCP API contract. It should enumerate each major endpoint/feature (components catalog, component schema, capabilities list, binding matrix, manifest validation, etc.) and state whether it’s Implemented, Stubbed, or Not Implemented, with brief commentary. For example: “/platform/components – Stub: Endpoint present but returns placeholder text, should list all components with their version and description as per spec.” This will guide developers on what to build next to achieve full MCP compliance.
PROMPT 11 — Security & Compliance Audit
Goal: Audit the platform components to ensure they implement “security by default” and comply with enterprise and FedRAMP High baseline requirements. Check that all infrastructure resources are hardened (encryption, private networking, least privilege) and that compliance features (like mandatory access logging, data classification tags, etc.) are either in place or enforced via policy. The aim is to ensure that simply using platform components yields a secure architecture without needing manual tweaks. Instructions:
Encryption & Access Controls: For each component that manages data or connectivity, verify encryption at rest and in transit is enforced:
S3 Bucket Component: Confirm that new buckets have encryption enabled by default (SSE-S3 or KMS CMK if in FedRAMP mode) and block public access (no public ACLs or policies). Check if versioning is turned on by default. Look in the S3 component’s code or defaults for properties like encryption: BucketEncryption.*, publicReadAccess: false, blockPublicAccess: BlockPublicAccess.BLOCK_ALL, versioned: true.
RDS/Postgres Component: Ensure that the RDS instances are launched with storage encryption enabled (and using a customer-managed CMK in FedRAMP contexts). Check for storageEncrypted: true and kmsKeyId usage in FedRAMP. Confirm deletionProtection: true by default and that the instance is not publicly accessible (no public subnets unless explicitly overridden). Also verify that if FedRAMP-high, settings like SSL-only/TLS and audit logging are considered (even if via parameter groups or a note to user).
Lambda & Compute Components: Verify that Lambda functions run inside a VPC by default (to ensure network control) – e.g. the Lambda component likely attaches to the service VPC if not told otherwise. Ensure IAM policies granted are scoped (least privilege) and that environment variables do not contain secrets in plaintext (use Secrets Manager integration instead). Also check that Lambda has concurrency limits or timeouts set reasonably (avoid unlimited runtime which can incur cost/security issues).
SQS Queue Component: Check that SQS queues are encrypted (AWS-managed KMS by default) and that a Dead-Letter Queue (DLQ) is automatically created and configured for each queue (to avoid message loss). See if the SQS component’s builder always adds a DLQ with a maxReceiveCount.
DynamoDB Component: Ensure DynamoDB tables have encryption enabled (should be by default with AWS-owned or KMS). If applicable, verify point-in-time recovery (PITR) is enabled for resilience. This might not be auto-set in code yet; if not, mention as a recommendation.
General Networking: Any component dealing with networking (VPC, Security Groups, etc.) should default to least privilege (e.g. no 0.0.0.0/0 ingress unless absolutely necessary). If a VPC component exists, ensure it doesn’t open broad access. Security group rules in binder strategies should be scoped to the required ports between components.
Logging & Monitoring (Compliance): Ensure that for sensitive resources, logging is enabled:
S3: Server access logging or CloudTrail data events for buckets should be enabled especially in FedRAMP mode. If the S3 component does not itself enable access logs to a log bucket, ensure the platform’s policy (OPA or nag) flags it.
RDS: Check if there is support for auditing (e.g. whether RDS Enhanced Monitoring or CloudWatch exports for audit logs can be enabled). If not directly in code, note if FedRAMP requires it as a backlog item.
CloudWatch Alarms: Confirm if the platform sets up any default CloudWatch alarms or if not, that it’s noted as future work (so that critical failures don’t go unnoticed in higher environments).
Compliance Framework Adjustments: Determine how the platform changes behavior under different complianceFramework settings:
For FedRAMP Moderate/High, are additional controls applied? e.g. using CMK instead of AWS-managed keys, enforcing TLS 1.2+, requiring all services be in approved regions, etc. Look at how the fedramp-high.yml defaults differ from commercial.yml. For example, FedRAMP might require a KMS CMK ARN for S3 encryption – check if the config supports injecting that.
Check if components read the complianceFramework and adjust. For instance, RDS might have conditional logic: if framework is fedramp-high, enable performance insights with KMS, etc. If code doesn’t currently handle that, highlight the gap (the defaults YAML may handle some of it).
Validate that compliance tags (like data classification and compliance framework tags) are enforced. Tagging audit (Prompt 02) covers tags in general, but here ensure specifically that when complianceFramework is set to fedramp, the system auto-tags resources with that info (or at least the stack). If the mechanism is via environment config, mention it.
Least Privilege & IAM: Review any IAM roles/policies the platform generates (e.g. IAM Role component or policies attached via binder strategies). Ensure policies are scoped to only required actions. For example, the Database binder should grant a lambda only the minimal AWS Secrets Manager read for its DB credentials, not broader permissions. If possible, inspect a sample synthesized policy (maybe via unit tests or by using cdk synth on a simple manifest). Identify any overly broad policy (like wildcard resource or actions) and note it. Also ensure no default credentials or secrets are hardcoded anywhere (which should not be, likely using Secrets Manager).
Output & Recommendations: Summarize how well each component meets security baseline: e.g. “S3Bucket: Compliant (private, encrypted, versioned by default; enable access logs in FedRAMP via policy)”, “RDS: Mostly Compliant (encryption and deletion protection on; FedRAMP audit logging needs external config)”. Highlight any critical gaps (things that could cause a compliance finding), such as a resource not being encrypted by default or a network exposure. Provide suggestions for those (e.g. “Enable SSE-KMS with a CMK for all FedRAMP S3 buckets – currently uses AES256 only”).
Deliverable: reports/security_compliance_audit.md – A comprehensive Markdown report organized by component or resource type, detailing adherence to security standards. It should call out each major control (encryption, network, logging, IAM) and whether the platform implements it by construction. Use bullet points for each component or control area. For any shortfall, clearly state what is missing and possibly how it should be addressed (since this can inform future enhancements or policies). If many items are compliant by default, emphasize the strengths of the platform’s secure-by-design approach as well.